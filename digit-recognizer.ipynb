{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit recognizer - CNN model trained on MNIST database using TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slawomirkapka/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, transform data and split it into train-val sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No test set?! How could it be?! Kaggle provides test set and the way to check the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "train_labels = df_train[df_train.columns[0]]\n",
    "y_train = train_labels.values\n",
    "\n",
    "df_train_values = df_train.drop(['label'], axis=1)\n",
    "X_train = df_train_values.values\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting data into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "def reshape(X, y):\n",
    "    X_reshaped = tf.reshape(X, [28, 28, 1])\n",
    "    return (X_reshaped, y)\n",
    "\n",
    "train_ds_reshaped = train_ds.map(reshape)\n",
    "\n",
    "## Transformations\n",
    "\n",
    "def zoom_out(X):\n",
    "    rand_tsr = tf.random_uniform(shape = (), minval=24, maxval=28, dtype = tf.int32)\n",
    "    smaller_size_tsr = tf.stack([rand_tsr, rand_tsr])\n",
    "    X_zoomed = tf.image.resize_images(X, smaller_size_tsr)\n",
    "    smaller_padding = tf.cast(tf.divide(tf.subtract(28, rand_tsr), 2), dtype=tf.int32)\n",
    "    bigger_padding = tf.cast(tf.divide(tf.subtract(29, rand_tsr), 2), dtype=tf.int32)\n",
    "    padding = tf.stack([[smaller_padding, bigger_padding], [smaller_padding, bigger_padding], [0, 0]])\n",
    "    X_padded = tf.pad(X_zoomed, padding)\n",
    "    return X_padded\n",
    "\n",
    "def zoom_in(X):\n",
    "    rand_tsr = tf.random_uniform(shape = (), minval=29, maxval=33, dtype = tf.int32)\n",
    "    bigger_size_tsr = tf.stack([rand_tsr, rand_tsr])\n",
    "    X_zoomed = tf.image.resize_images(X, bigger_size_tsr) \n",
    "    offset = tf.cast(tf.divide(tf.subtract(rand_tsr, 28), 2), dtype=tf.int32)  \n",
    "    X_cropped = tf.image.crop_to_bounding_box(X_zoomed, offset, offset, 28, 28)   \n",
    "    return X_cropped\n",
    "\n",
    "def rotate(X):\n",
    "    rand_tsr = tf.random_uniform(shape = (), minval=-0.5, maxval=0.5)\n",
    "    X_rotated = tf.contrib.image.rotate(X, rand_tsr)   \n",
    "    return X_rotated\n",
    "\n",
    "def random_transform(X, y):\n",
    "    rand_tsr = tf.random_uniform(shape = (), minval=0, maxval=2, dtype = tf.int32)\n",
    "    X_transformed = tf.cond(tf.equal(rand_tsr, tf.constant(0)), lambda: zoom_out(rotate(X)), lambda: zoom_in(rotate(X)))\n",
    "    return (X_transformed, y)\n",
    "\n",
    "train_ds_transformed = train_ds_reshaped.map(random_transform)\n",
    "train_ds_shuffled = train_ds_transformed.shuffle(37800)\n",
    "train_ds_batched = train_ds_shuffled.batch(100)\n",
    "train_iterator = train_ds_batched.make_one_shot_iterator()\n",
    "initialise_train = train_iterator.make_initializer(train_ds_batched)\n",
    "train_element = train_iterator.get_next()\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds_reshaped = val_ds.map(reshape)\n",
    "\n",
    "val_ds_one_batch = val_ds_reshaped.batch(4200)\n",
    "val_iterator = val_ds_one_batch.make_one_shot_iterator()\n",
    "initialise_val = val_iterator.make_initializer(val_ds_one_batch)\n",
    "val_element = val_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "prob_conv = tf.placeholder_with_default(1.0, shape=())\n",
    "prob_dense = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "# Spaghetti code. Prone to errors. Needs refactorisation!\n",
    "with tf.name_scope('Model'):\n",
    "    with tf.name_scope('Input_L'):\n",
    "        input_tsr = tf.reshape(x, [-1, 28, 28, 1], name='InputReshaped')\n",
    "    with tf.name_scope('Conv1_L'):\n",
    "        W_conv1_1 = tf.get_variable(\"W_conv1_1\", [5, 5, 1, 32])\n",
    "        b_conv1_1 = tf.get_variable(\"b_conv1_1\", [32])\n",
    "        W_conv1_1_his = tf.summary.histogram(\"W_conv1_1\", W_conv1_1)\n",
    "        b_conv1_1_his = tf.summary.histogram(\"b_conv1_1\", b_conv1_1)\n",
    "        conv1_1 = tf.add(tf.nn.conv2d(input_tsr, W_conv1_1, strides=[1, 1, 1, 1], padding='SAME'), b_conv1_1)\n",
    "        h_conv1_1 = tf.nn.relu(conv1_1)\n",
    "        W_conv1_2 = tf.get_variable(\"W_conv1_2\", [4, 4, 32, 32])\n",
    "        b_conv1_2 = tf.get_variable(\"b_conv1_2\", [32])\n",
    "        W_conv1_2_his = tf.summary.histogram(\"W_conv1_2\", W_conv1_2)\n",
    "        b_conv1_2_his = tf.summary.histogram(\"b_conv1_2\", b_conv1_2)\n",
    "        conv1_2 = tf.add(tf.nn.conv2d(h_conv1_1, W_conv1_2, strides=[1, 1, 1, 1], padding='SAME'), b_conv1_2)\n",
    "        h_conv1_2 = tf.nn.relu(conv1_2)\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        dropout1 = tf.nn.dropout(h_pool1, prob_conv)\n",
    "    with tf.name_scope('Conv2_L'):\n",
    "        W_conv2_1 = tf.get_variable(\"W_conv2_1\", [3, 3, 32, 64])\n",
    "        b_conv2_1 = tf.get_variable(\"b_conv2_1\", [64])\n",
    "        W_conv2_1_his = tf.summary.histogram(\"W_conv2_1\", W_conv2_1)\n",
    "        b_conv2_1_his = tf.summary.histogram(\"b_conv2_1\", b_conv2_1)\n",
    "        conv2_1 = tf.add(tf.nn.conv2d(dropout1, W_conv2_1, strides=[1, 1, 1, 1], padding='SAME'), b_conv2_1)\n",
    "        h_conv2_1 = tf.nn.relu(conv2_1)\n",
    "        W_conv2_2 = tf.get_variable(\"W_conv2_2\", [2, 2, 64, 64])\n",
    "        b_conv2_2 = tf.get_variable(\"b_conv2_2\", [64])\n",
    "        W_conv2_2_his = tf.summary.histogram(\"W_conv2_2\", W_conv2_2)\n",
    "        b_conv2_2_his = tf.summary.histogram(\"b_conv2_2\", b_conv2_2)\n",
    "        conv2_2 = tf.add(tf.nn.conv2d(h_conv2_1, W_conv2_2, strides=[1, 1, 1, 1], padding='SAME'), b_conv2_2)\n",
    "        h_conv2_2 = tf.nn.relu(conv2_2)\n",
    "        h_pool2 = tf.nn.max_pool(h_conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        dropout2 = tf.nn.dropout(h_pool2, prob_conv)\n",
    "    with tf.name_scope('Dense_L'):\n",
    "        W_dense = tf.get_variable(\"W_dense\", [7 * 7 * 64, 256])\n",
    "        b_dense = tf.get_variable(\"b_dense\", [256])\n",
    "        W_dense_his = tf.summary.histogram(\"W_dense\", W_dense)\n",
    "        b_dense_his = tf.summary.histogram(\"b_dense\", b_dense)\n",
    "        flat_tsr = tf.reshape(dropout2, [-1, 7 * 7 * 64])\n",
    "        dense = tf.add(tf.matmul(flat_tsr, W_dense), b_dense)\n",
    "        h_dense =  tf.nn.relu(dense)\n",
    "        dropout3 = tf.nn.dropout(h_dense, prob_dense)\n",
    "    with tf.name_scope('Output_L'):\n",
    "        W_out = tf.get_variable(\"W_out\", [256, 10])\n",
    "        b_out = tf.get_variable(\"b_out\", [10])\n",
    "        W_out_his = tf.summary.histogram(\"W_out\", W_out)\n",
    "        b_out_his = tf.summary.histogram(\"b_out\", b_out)\n",
    "        pred = tf.add(tf.matmul(dropout3, W_out), b_out)\n",
    "        \n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "    \n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_last_batch = tf.summary.scalar(\"Loss_on_last_batch\", loss)\n",
    "acc_last_batch = tf.summary.scalar(\"Accuracy_on_last_batch\", acc)\n",
    "merged_summaries_from_last_batch = tf.summary.merge([loss_last_batch, acc_last_batch])\n",
    "\n",
    "validation_set_acc = tf.summary.scalar(\"Validation_set_accuracy\", acc)\n",
    "merged_validation = tf.summary.merge([validation_set_acc,  \n",
    "                                      W_conv1_1_his, b_conv1_1_his, W_conv1_2_his, b_conv1_2_his,\n",
    "                                      W_conv2_1_his, b_conv2_1_his, W_conv2_2_his, b_conv2_2_his,\n",
    "                                      W_dense_his, b_dense_his,\n",
    "                                      W_out_his, b_out_his])\n",
    "\n",
    "logs_path = 'logs'\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "total_batch = 378\n",
    "save_path = \"saved_models/saved_model.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run session and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 done in 124.63 sec, avg_cost = 0.994784897, avg_acc = 0.7322\n",
      "Epoch: 0002 done in 109.07 sec, avg_cost = 0.244456088, avg_acc = 0.9259\n",
      "Epoch: 0003 done in 107.40 sec, avg_cost = 0.187155464, avg_acc = 0.9436\n",
      "Epoch: 0004 done in 108.16 sec, avg_cost = 0.155999397, avg_acc = 0.9534\n",
      "Epoch: 0005 done in 116.12 sec, avg_cost = 0.138635470, avg_acc = 0.9603\n",
      "Epoch: 0006 done in 110.85 sec, avg_cost = 0.131608163, avg_acc = 0.9605\n",
      "Epoch: 0007 done in 111.54 sec, avg_cost = 0.119466926, avg_acc = 0.9641\n",
      "Epoch: 0008 done in 108.88 sec, avg_cost = 0.119532470, avg_acc = 0.9638\n",
      "Epoch: 0009 done in 109.47 sec, avg_cost = 0.113237058, avg_acc = 0.9661\n",
      "Epoch: 0010 done in 110.56 sec, avg_cost = 0.106664859, avg_acc = 0.9688\n",
      "Epoch: 0011 done in 110.45 sec, avg_cost = 0.103215413, avg_acc = 0.9701\n",
      "Epoch: 0012 done in 110.97 sec, avg_cost = 0.094025335, avg_acc = 0.9722\n",
      "Epoch: 0013 done in 109.92 sec, avg_cost = 0.100626250, avg_acc = 0.9698\n",
      "Epoch: 0014 done in 108.64 sec, avg_cost = 0.091275956, avg_acc = 0.9729\n",
      "Epoch: 0015 done in 108.37 sec, avg_cost = 0.092558342, avg_acc = 0.9727\n",
      "Epoch: 0016 done in 107.06 sec, avg_cost = 0.086209887, avg_acc = 0.9745\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        time_start = time.time()\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        sess.run(initialise_train)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = sess.run(train_element)\n",
    "            _, c, ac = sess.run([train_step, loss, acc], feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                                                    prob_conv: 0.75, prob_dense: 0.5})\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += ac / total_batch\n",
    "            \n",
    "        summary_last_batch = sess.run(merged_summaries_from_last_batch, feed_dict={x: batch_xs, y: batch_ys, \n",
    "                                                                                   prob_conv: 0.75, prob_dense: 0.5})\n",
    "        summary_writer.add_summary(summary_last_batch, epoch + 1)\n",
    "        \n",
    "        sess.run(initialise_val)\n",
    "        X_val_pl, y_val_pl = sess.run(val_element)\n",
    "        summary_validation = sess.run(merged_validation, feed_dict={x: X_val_pl, y: y_val_pl,\n",
    "                                                                    prob_conv: 1., prob_dense: 1.})\n",
    "        summary_writer.add_summary(summary_validation, epoch + 1)\n",
    "        time_elapsed = time.time() - time_start\n",
    "        print(\"Epoch: {:04} done in {:.2f} sec, avg_cost = {:.9f}, avg_acc = {:.4f}\".format(epoch + 1, time_elapsed, avg_cost, avg_acc ))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    saver.save(sess, save_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_training_epochs = 50\n",
    "batch_size = 100\n",
    "total_batch = 378\n",
    "save_path = \"saved_models/restored_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_models/saved_model.ckpt\n",
      "Epoch: 0051 done in 129.89 sec, avg_cost = 0.064966585, avg_acc = 0.9808\n",
      "Epoch: 0052 done in 109.32 sec, avg_cost = 0.067310688, avg_acc = 0.9812\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, \"saved_models/saved_model.ckpt\")\n",
    "\n",
    "    for epoch in range(continue_training_epochs):\n",
    "        time_start = time.time()\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        sess.run(initialise_train)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = sess.run(train_element)\n",
    "            _, c, ac = sess.run([train_step, loss, acc], feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                                                    prob_conv: 0.75, prob_dense: 0.5})\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += ac / total_batch\n",
    "            \n",
    "        summary_last_batch = sess.run(merged_summaries_from_last_batch, feed_dict={x: batch_xs, y: batch_ys, \n",
    "                                                                                   prob_conv: 0.75, prob_dense: 0.5})\n",
    "        summary_writer.add_summary(summary_last_batch, epoch + training_epochs + 1)\n",
    "        \n",
    "        sess.run(initialise_val)\n",
    "        X_val_pl, y_val_pl = sess.run(val_element)\n",
    "        summary_validation = sess.run(merged_validation, feed_dict={x: X_val_pl, y: y_val_pl,\n",
    "                                                                    prob_conv: 1., prob_dense: 1.})\n",
    "        summary_writer.add_summary(summary_validation, epoch + training_epochs + 1)\n",
    "        time_elapsed = time.time() - time_start\n",
    "        print(\"Epoch: {:04} done in {:.2f} sec, avg_cost = {:.9f}, avg_acc = {:.4f}\".format(epoch + training_epochs + 1, time_elapsed, avg_cost, avg_acc ))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    saver.save(sess, save_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
